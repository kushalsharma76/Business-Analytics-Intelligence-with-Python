{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   R&D Spend  Administration  Marketing Spend       State     Profit\n",
      "0  165349.20       136897.80        471784.10    New York  192261.83\n",
      "1  162597.70       151377.59        443898.53  California  191792.06\n",
      "2  153441.51       101145.55        407934.54     Florida  191050.39\n",
      "3  144372.41       118671.85        383199.62    New York  182901.99\n",
      "4  142107.34        91391.77        366168.42     Florida  166187.94\n",
      "[[1.0000000e+00 0.0000000e+00 6.6051520e+04 1.8264556e+05 1.1814820e+05]\n",
      " [0.0000000e+00 0.0000000e+00 1.0067196e+05 9.1790610e+04 2.4974455e+05]\n",
      " [1.0000000e+00 0.0000000e+00 1.0191308e+05 1.1059411e+05 2.2916095e+05]\n",
      " [1.0000000e+00 0.0000000e+00 2.7892920e+04 8.4710770e+04 1.6447071e+05]\n",
      " [1.0000000e+00 0.0000000e+00 1.5344151e+05 1.0114555e+05 4.0793454e+05]\n",
      " [0.0000000e+00 1.0000000e+00 7.2107600e+04 1.2786455e+05 3.5318381e+05]\n",
      " [0.0000000e+00 1.0000000e+00 2.0229590e+04 6.5947930e+04 1.8526510e+05]\n",
      " [0.0000000e+00 1.0000000e+00 6.1136380e+04 1.5270192e+05 8.8218230e+04]\n",
      " [1.0000000e+00 0.0000000e+00 7.3994560e+04 1.2278275e+05 3.0331926e+05]\n",
      " [1.0000000e+00 0.0000000e+00 1.4210734e+05 9.1391770e+04 3.6616842e+05]]\n",
      "[140264.53365313  93013.83044965 102808.58105425  89353.12448619\n",
      "  97896.74630994 111766.87132106  79579.51098615 124678.58029293\n",
      " 109144.84662676  92826.24292188]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Prushaltech\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:371: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\Prushaltech\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:392: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# Multiple Linear Regression\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the datasets\n",
    "\n",
    "datasets = pd.read_csv('50_Startups.csv')\n",
    "print(datasets.head())\n",
    "\n",
    "# Separating the datasets\n",
    "X = datasets.iloc[:, :-1].values\n",
    "Y = datasets.iloc[:, 4].values\n",
    "\n",
    "# Encoding categorical data\n",
    "\n",
    "# Encoding the Independent Variable\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "#Label Encoder vs. One Hot Encoder in Machine Learning. ... These two encoders are parts of the SciKit Learn library in Python, and they are used to convert categorical data, or text data, into numbers, which our predictive models can better understand\n",
    "X[:, 3] = labelencoder_X.fit_transform(X[:, 3])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [3])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "# Avoiding the Dummy Variable Trap\n",
    "X = X[:, 1:]\n",
    "# print(\"Dummy Variable Trap\", X)\n",
    "#Dummy Variable Trap in Regression Models. ... The Dummy Variable trap is a scenario in which the independent variables are multicollinear - a scenario in which two or more variables are highly correlated; in simple terms one variable can be predicted from the others.\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_Train, X_Test, Y_Train, Y_Test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Fitting the Multiple Linear Regression in the Training set\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_Train, Y_Train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "\n",
    "Y_Pred = regressor.predict(X_Test)\n",
    "\n",
    "# Building the optimal model using Backward Elimination\n",
    "#Backward Elimination: In backward elimination, we start with all the features and removes the least significant feature at each iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of features.\n",
    "import statsmodels.formula.api as sm\n",
    "X = np.append(arr = np.ones((50, 1)).astype(int), values = X, axis = 1)\n",
    "\n",
    "X_Optimal = X[:, [0,1,2,3,4,5]]\n",
    "regressor_OLS = sm.OLS(endog = Y, exog = X_Optimal).fit()\n",
    "regressor_OLS.summary()\n",
    "\n",
    "X_Optimal = X[:, [0,1,2,4,5]]\n",
    "regressor_OLS = sm.OLS(endog = Y, exog = X_Optimal).fit()\n",
    "regressor_OLS.summary()\n",
    "\n",
    "X_Optimal = X[:, [0,1,4,5]]\n",
    "regressor_OLS = sm.OLS(endog = Y, exog = X_Optimal).fit()\n",
    "regressor_OLS.summary()\n",
    "\n",
    "X_Optimal = X[:, [0,1,4]]\n",
    "regressor_OLS = sm.OLS(endog = Y, exog = X_Optimal).fit()\n",
    "regressor_OLS.summary()\n",
    "\n",
    "# Fitting the Multiple Linear Regression in the Optimal Training set\n",
    "\n",
    "X_Optimal_Train, X_Optimal_Test = train_test_split(X_Optimal,test_size = 0.2, random_state = 0)\n",
    "regressor.fit(X_Optimal_Train, Y_Train)\n",
    "\n",
    "# Predicting the Optimal Test set results\n",
    "\n",
    "Y_Optimal_Pred = regressor.predict(X_Optimal_Test)\n",
    "# print(X_Optimal_Test)\n",
    "# print(Y_Optimal_Pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
